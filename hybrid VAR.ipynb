{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (0.31.2)\n",
      "Requirement already satisfied: requests in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\poova\\desktop\\intellimanthan competition\\research paper\\var\\varenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poova\\Desktop\\Intellimanthan Competition\\Research paper\\VAR\\varenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.7.0+cpu)\n",
      "    Python  3.10.11 (you have 3.10.0)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install huggingface_hub requests\n",
    "import time\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "from models import VQVAE, build_vae_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable default parameter initialization for faster speed\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model depth configuration\n",
    "MODEL_DEPTH = 16  # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}, \"Invalid MODEL_DEPTH value!\"\n",
    "\n",
    "# Define checkpoint URLs\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt = 'vae_ch160v4096z32.pth'\n",
    "var_ckpt = f'var_d{MODEL_DEPTH}.pth'\n",
    "\n",
    "# Function to download files if not already present\n",
    "def download_file(url, filename):\n",
    "    if not osp.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Download checkpoints\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "vae_ckpt_path = osp.join(\"checkpoints\", vae_ckpt)\n",
    "var_ckpt_path = osp.join(\"checkpoints\", var_ckpt)\n",
    "download_file(f\"{hf_home}/{vae_ckpt}\", vae_ckpt)\n",
    "download_file(f\"{hf_home}/{var_ckpt}\", var_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n"
     ]
    }
   ],
   "source": [
    "# Auto-select device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build models\n",
    "vae, var = build_vae_var(\n",
    "    V=4096,\n",
    "    Cvae=32,\n",
    "    ch=160,\n",
    "    share_quant_resi=4,\n",
    "    device=device,\n",
    "    patch_nums=(1, 2, 3, 4, 5, 6, 8, 10, 13, 16),\n",
    "    num_classes=1000,\n",
    "    depth=MODEL_DEPTH,\n",
    "    shared_aln=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "\n",
    "# Load VAR model checkpoint with strict=False to allow mismatched parameters\n",
    "var_checkpoint = torch.load(var_ckpt, map_location='cpu')\n",
    "\n",
    "# Filter out mismatched keys\n",
    "filtered_checkpoint = {k: v for k, v in var_checkpoint.items() if k in var.state_dict() and var.state_dict()[k].shape == v.shape}\n",
    "\n",
    "# Load the filtered checkpoint\n",
    "var.load_state_dict(filtered_checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log skipped parameters\n",
    "missing_keys, unexpected_keys = var.load_state_dict(filtered_checkpoint, strict=False)\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"Missing keys in the checkpoint: {missing_keys}\")\n",
    "    \n",
    "if unexpected_keys:\n",
    "    print(f\"Unexpected keys in the checkpoint: {unexpected_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle mismatched parameters by initializing them with default values\n",
    "for key in missing_keys:\n",
    "    if key in var.state_dict():\n",
    "        print(f\"Initializing missing key: {key}\")\n",
    "        var.state_dict()[key].copy_(torch.zeros_like(var.state_dict()[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model preparation finished.\n"
     ]
    }
   ],
   "source": [
    "# Set models to evaluation mode and freeze parameters\n",
    "vae.eval()\n",
    "var.eval()\n",
    "\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in var.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "print(\"Model preparation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Set arguments\n",
    "seed = 0  \n",
    "num_sampling_steps = 250  \n",
    "cfg = 4\n",
    "class_labels = (43, 113, 134)  \n",
    "more_smooth = False \n",
    "num_diffusion_steps = 50\n",
    "noise_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TensorFloat-32 for faster computation\n",
    "tf32 = True \n",
    "torch.backends.cudnn.allow_tf32 = tf32\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling process setup\n",
    "B = len(class_labels)\n",
    "label_B = torch.tensor(class_labels, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 294.96 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Run the hybrid model\n",
    "with torch.inference_mode():\n",
    "    recon_B3HW = var.autoregressive_infer_cfg(\n",
    "        B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth\n",
    "    )\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_time = end - start\n",
    "\n",
    "print(f\"Inference Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "grid = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "grid = grid.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "PImage.fromarray(grid.astype(np.uint8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved pseudo diffusion model\n",
    "class DiffusionModel:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def refine(self, structure, num_steps=num_diffusion_steps, noise_scale=noise_scale):\n",
    "        # Normalize to [0, 1]\n",
    "        structure = torch.clamp(structure, 0, 1)\n",
    "\n",
    "        # Improved refinement loop for better clarity\n",
    "        noisy = structure.clone()\n",
    "        for step in range(num_steps):\n",
    "            noise = noise_scale * torch.randn_like(noisy)\n",
    "            noisy = torch.clamp(noisy + noise, 0, 1)  # Add slight noise\n",
    "            alpha = 0.5 * (1 - step / num_steps)  # Gradually reduce noise influence\n",
    "            noisy = noisy.lerp(structure, alpha)  # Gradually refine towards the original structure\n",
    "\n",
    "        return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hybrid AR-Diffusion architecture\n",
    "class HybridARDiffusion:\n",
    "    def __init__(self, ar_model, diffusion_model, device):\n",
    "        self.ar_model = ar_model\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, class_labels, cfg, num_diffusion_steps, top_k=900, top_p=0.95, noise_scale=noise_scale):\n",
    "        # Step 1: Generate coarse structure using AR model\n",
    "        with torch.inference_mode():\n",
    "            structure = self.ar_model.autoregressive_infer_cfg(\n",
    "                B=B, label_B=label_B, cfg=cfg, top_k=top_k, top_p=top_p\n",
    "            )\n",
    "\n",
    "        # Step 2: Refine the structure using the diffusion model\n",
    "        refined_output = self.diffusion_model.refine(structure, num_steps=num_diffusion_steps, noise_scale=noise_scale)\n",
    "        return refined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the hybrid model\n",
    "hybrid_model = HybridARDiffusion(var, DiffusionModel(device), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 343.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate refined images and calculate inference time\n",
    "start = time.time()\n",
    "\n",
    "refined_images = hybrid_model.generate(class_labels, cfg, num_diffusion_steps=num_diffusion_steps, noise_scale=noise_scale)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_time = end - start\n",
    "\n",
    "print(f\"Inference Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "grid = torchvision.utils.make_grid(refined_images, nrow=8, padding=0, pad_value=1.0)\n",
    "grid = grid.permute(1, 2, 0).mul(255).byte().cpu().numpy()\n",
    "PImage.fromarray(grid).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
